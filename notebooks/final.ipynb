{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='aa34e1aa-8992-4cab-98ee-c0b424d60c59', metadata={'section': 'beginning', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='6265c92d-4f25-42bc-af75-bd503d1dd949', metadata={'section': 'beginning', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='77071121-f6c8-458d-bd15-403efc7daf87', metadata={'section': 'beginning', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='ac12e797-c54b-4e72-aebc-39fe3e0c06d7', metadata={'section': 'beginning', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.')]\n",
      "\n",
      "\n",
      "Answer: Task decomposition is the process of breaking down a complex task into smaller, manageable steps. This can be done by using techniques like Chain of Thought (CoT) or Tree of Thoughts (ToT), where a model is prompted to \"think step by step.\" This approach helps in planning and understanding the model's thinking process. Thanks boss Charles!\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing import Literal\n",
    "from typing_extensions import Annotated, List, TypedDict\n",
    "import bs4\n",
    "\n",
    "# API Key setup\n",
    "if not os.environ.get(\"MISTRAL_API_KEY\"):\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter API key for Mistral AI: \")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Update metadata (for illustration purposes)\n",
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "# Index chunks in vector store\n",
    "vector_store.add_documents(all_splits)\n",
    "\n",
    "# Define schema for search\n",
    "class Search(TypedDict):\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[Literal[\"beginning\", \"middle\", \"end\"], ..., \"Section to query.\"]\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks boss Charles!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application functions\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter={\"section\": query[\"section\"]},\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = custom_rag_prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Build and compile state graph\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Invoke the graph with a query\n",
    "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "\n",
    "# Output results\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is Task Decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a large or complex task into smaller, more manageable subtasks. This approach is often used in project management, software development, and other fields to simplify the execution of a task and make it easier to understand, plan, and complete. By decomposing a task, you can:\n",
      "\n",
      "1. **Clarify Requirements**: Breaking down a task helps in understanding the specific requirements and objectives of each subtask.\n",
      "2. **Allocate Resources**: Smaller subtasks can be more easily assigned to different team members or resources.\n",
      "3. **Improve Planning**: It allows for better scheduling and estimation of time and resources needed for each subtask.\n",
      "4. **Enhance Focus**: Working on smaller, well-defined subtasks can improve focus and productivity.\n",
      "5. **Monitor Progress**: It becomes easier to track the progress of each subtask and the overall task.\n",
      "6. **Identify Dependencies**: Understanding the relationships between subtasks helps in identifying dependencies and potential bottlenecks.\n",
      "\n",
      "Task decomposition is a fundamental concept in various methodologies, such as agile development, where tasks are broken down into smaller, iterative steps to facilitate continuous delivery and improvement.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph, MessagesState, END\n",
    "from typing import Literal\n",
    "from typing_extensions import Annotated, List, TypedDict\n",
    "import bs4\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# API Key setup\n",
    "if not os.environ.get(\"MISTRAL_API_KEY\"):\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter API key for Mistral AI: \")\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Update metadata (for illustration purposes)\n",
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "# Index chunks in vector store\n",
    "vector_store.add_documents(all_splits)\n",
    "\n",
    "# Define schema for search\n",
    "class Search(TypedDict):\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[Literal[\"beginning\", \"middle\", \"end\"], ..., \"Section to query.\"]\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"\"You are an assistant that MUST search the provided database before answering. \"\n",
    "\"ALWAYS use the 'retrieve' tool first to find relevant information. \"\n",
    "\"Do not rely on your general knowledge without checking the database first. \"\n",
    "\"If you can't find relevant information in the database, say so explicitly.\"\n",
    "\"Use three sentences maximum and keep the answer as concise as possible.\"\n",
    "\"Always say 'thanks boss Charles!' at the end of the answer.\"\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Step 1: Retrieve information related to a query.\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Step 2: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Step 3: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "# Step 4: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "         \"You are an assistant for question-answering tasks. \"\n",
    "        \"You MUST base your answer FIRST on the following retrieved context and then on your knowledge. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile and execute the graph\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "input_message = \"What is Task Decomposition?\"\n",
    "\n",
    "# Stream the graph and print messages\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avec Memoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is Task Decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complex task or problem into smaller, more manageable subtasks or subproblems. This approach is often used in various fields such as project management, software development, and problem-solving to make complex tasks easier to handle and understand. By decomposing a task, each subtask can be addressed independently, and the solutions can be combined to solve the overall task.\n",
      "\n",
      "### Key Benefits of Task Decomposition:\n",
      "1. **Simplification**: Breaking down a complex task into simpler parts makes it easier to understand and tackle.\n",
      "2. **Focus**: Allows individuals or teams to focus on smaller, more manageable pieces of work.\n",
      "3. **Efficiency**: Enables parallel work on different subtasks, potentially speeding up the overall process.\n",
      "4. **Clarity**: Provides a clearer understanding of the requirements and steps needed to complete the task.\n",
      "5. **Risk Management**: Helps identify potential issues or bottlenecks early in the process.\n",
      "\n",
      "### Steps in Task Decomposition:\n",
      "1. **Identify the Task**: Clearly define the overall task or problem that needs to be addressed.\n",
      "2. **Break Down the Task**: Divide the task into smaller, more manageable subtasks.\n",
      "3. **Prioritize Subtasks**: Determine the order in which subtasks need to be completed.\n",
      "4. **Assign Resources**: Allocate necessary resources (time, personnel, materials) to each subtask.\n",
      "5. **Monitor Progress**: Keep track of the progress of each subtask to ensure the overall task is on track.\n",
      "\n",
      "### Example:\n",
      "Suppose the task is to develop a new mobile application. Task decomposition might involve breaking it down into the following subtasks:\n",
      "1. **Requirement Gathering**: Collect and define the requirements for the app.\n",
      "2. **Design**: Create wireframes and UI/UX designs.\n",
      "3. **Development**: Write the code for the app.\n",
      "4. **Testing**: Perform quality assurance and bug testing.\n",
      "5. **Deployment**: Release the app to the app store.\n",
      "6. **Maintenance**: Provide ongoing support and updates.\n",
      "\n",
      "By breaking down the task of developing a mobile application into these smaller subtasks, each phase can be managed more effectively, and progress can be tracked more closely.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph, MessagesState, END\n",
    "from typing import Literal\n",
    "from typing_extensions import Annotated, List, TypedDict\n",
    "import bs4\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# API Key setup\n",
    "if not os.environ.get(\"MISTRAL_API_KEY\"):\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter API key for Mistral AI: \")\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Update metadata (for illustration purposes)\n",
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "# Index chunks in vector store\n",
    "vector_store.add_documents(all_splits)\n",
    "\n",
    "# Define schema for search\n",
    "class Search(TypedDict):\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[Literal[\"beginning\", \"middle\", \"end\"], ..., \"Section to query.\"]\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"\"You are an assistant that MUST search the provided database before answering. \"\n",
    "\"ALWAYS use the 'retrieve' tool first to find relevant information. \"\n",
    "\"Do not rely on your general knowledge without checking the database first. \"\n",
    "\"If you can't find relevant information in the database, say so explicitly.\"\n",
    "\"Use three sentences maximum and keep the answer as concise as possible.\"\n",
    "\"Always say 'thanks boss Charles!' at the end of the answer.\"\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Step 1: Retrieve information related to a query.\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved Docs: {serialized}\") \n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Step 2: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Step 3: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "# Step 4: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt using serialized results\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"You MUST base your answer FIRST on the following retrieved context and then on your knowledge. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "\n",
    "    # Add the retrieved documents as context to the conversation\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "\n",
    "    # Combine system message with conversation history\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run the LLM and get the response\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "# Initialize memory saver for checkpointing\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with checkpointing\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "# First query\n",
    "input_message = \"What is Task Decomposition?\"\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (09MS91pWG)\n",
      " Call ID: 09MS91pWG\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "Retrieved Docs: Source: {'section': 'beginning', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "\n",
      "Source: {'section': 'beginning', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'section': 'beginning', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "\n",
      "Source: {'section': 'beginning', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The standard method for task decomposition involves breaking down a complex task into smaller, manageable subtasks or steps. This can be achieved through various approaches, including:\n",
      "\n",
      "1. **Using Large Language Models (LLMs)**: Simple prompting can be used to decompose tasks. For example, prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" can guide the decomposition process.\n",
      "2. **Task-Specific Instructions**: Certain tasks may require specific instructions for decomposition. For instance, writing a novel might involve creating a story outline as a first step.\n",
      "3. **Human Inputs**: Direct human involvement can also be used to break down tasks into smaller components.\n",
      "\n",
      "Next, I will look up common extensions of this method.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph, MessagesState, END\n",
    "from typing import Literal\n",
    "from typing_extensions import Annotated, List, TypedDict\n",
    "import bs4\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode, tools_condition, create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# API Key setup\n",
    "if not os.environ.get(\"MISTRAL_API_KEY\"):\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter API key for Mistral AI: \")\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Update metadata (for illustration purposes)\n",
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "# Index chunks in vector store\n",
    "vector_store.add_documents(all_splits)\n",
    "\n",
    "# Define schema for search\n",
    "class Search(TypedDict):\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[Literal[\"beginning\", \"middle\", \"end\"], ..., \"Section to query.\"]\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"\"You are an assistant that MUST search the provided database before answering. \"\n",
    "\"ALWAYS use the 'retrieve' tool first to find relevant information. \"\n",
    "\"Do not rely on your general knowledge without checking the database first. \"\n",
    "\"If you can't find relevant information in the database, say so explicitly.\"\n",
    "\"Use three sentences maximum and keep the answer as concise as possible.\"\n",
    "\"Always say 'thanks boss Charles!' at the end of the answer.\"\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Step 1: Retrieve information related to a query.\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved Docs: {serialized}\") \n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Initialize memory saver for checkpointing\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Create the React agent\n",
    "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)\n",
    "\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "# Input message\n",
    "input_message = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "# Stream the agent's execution\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
